RNN 理论介绍
RNN 发展历史
循环神经网络（Recurrent Neural Networks，RNN）的发展历程可以追溯到1980年代。RNN 发展历程大概的概述如下：

1982年：John Hopfield 首次介绍了一种新型神经网络模型，即 Hopfield 网络。Hopfield 网络可以被视为 RNN 的起点，它使用全连接的神经元并且所有连接都有相同的权重，以此实现动态稳定性。

1986年：David Rumelhart, Geoffrey Hinton 和 Ronald Williams 提出了反向传播（backpropagation）算法。这个算法也被用于训练 RNN。

1989年：Elman 发表论文，首次提出了今天我们称为 Elman 网络或者简单RNN的网络结构。这种网络将前一步的隐藏状态作为当前步的输入的一部分，实现了对序列数据的处理。

1997年：Hochreiter 和 Schmidhuber 提出了长短期记忆（Long Short-Term Memory, LSTM）模型，这是一种特殊的 RNN，能够有效地解决梯度消失和梯度爆炸问题，以便学习长序列中的依赖关系。

2000年：Felix Gers 和他的同事对 LSTM 进行了改进，添加了“遗忘门”，进一步增强了模型的性能。

2014年：Cho 等人提出了门控循环单元（Gated Recurrent Unit, GRU）。GRU 是 LSTM 的一种变体，结构更简单，但保留了类似的性能。

2015年：由于计算机硬件的改进，尤其是 GPU 的使用，以及更大数据集的可用性，RNN，特别是其 LSTM 和 GRU 变体，在许多任务中（包括机器翻译、语音识别等）都取得了显著的成功。

近年来：随着注意力机制（attention）的提出，以及 Transformer 模型的出现，RNN在某些任务上（尤其是 NLP 任务）已被这些更新的架构所取代。然而，RNN 仍然在处理序列数据和建模时间依赖性方面有其独特的优势，仍在许多应用中被广泛使用。

基于这个发展历程，我联想到之前设想的关于深度学习的联想，例如参照人的记忆以及逻辑判断，是有遗忘属性的，那么这里的遗忘门是不是就是对应了这个?
以及长短期记忆，是不是参照人脑对既定的已知的知识或者认知框架对未来的判断的影响
以及transformer模型，这些大模型的变体，甚至是日后还会出现的新的深度学习框架
